{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First draft of Contrast algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "- get_cluster_analysis: try zero, relevant and inf dimensions again\n",
    "- Inverse law when seeing duplicates\n",
    "- Implement context\n",
    "- Recollection: random updates vs complete updates?\n",
    "- Hub clusters\n",
    "- Forgetting data\n",
    "\n",
    "## Not to do\n",
    "- Implement a tree-like structure for clusters (not relevant if we update\n",
    "  only one outlier point we saw early)\n",
    "- When updating st_dists of cluster, project along both infinite-stdev and\n",
    "  0-stdev dims\n",
    "- Update sensitivity -> how?\n",
    "- Compare stdevs and distances\n",
    "\n",
    "## Done\n",
    "- ✔ Compute standard_distances for each cluster\n",
    "- ✔ Clustering validity checking methods (Note: not necessarily relevant\n",
    "  though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import csv\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import special_ortho_group\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn import metrics\n",
    "from utils import plot_confusion_matrix\n",
    "import collections\n",
    "import functools\n",
    "import itertools\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FEATURES = 5\n",
    "NB_GROUPS = 5\n",
    "N = 300\n",
    "DOMAIN_LENGTH = 200\n",
    "DEV_MAX = 20\n",
    "\n",
    "COLORS = ['r', 'g', 'b', 'y', 'c', 'm']\n",
    "\n",
    "# stocks metadata as (DATASET_NAME, DATASET_EXTENSION, DATASET_PATH,\n",
    "#     \\ DATASET_CLUSTER_COLUMN_INDEX, DATASET_DATA_COLUMNS_INDICES)\n",
    "METADATA = {\n",
    "    'Cards': ('Cards', '.csv', 'data/', 1, (2, None)),\n",
    "    'Cards_truncated': ('Cards', '.csv', 'data/', 1, (2, 7))\n",
    "}\n",
    "\n",
    "SHOULD_LOAD_DATASET = 1  # 0 to generate, 1 to load\n",
    "if SHOULD_LOAD_DATASET:\n",
    "    NAME = 'Cards'\n",
    "    DATASET_NAME, DATASET_EXTENSION, DATASET_PATH, \\\n",
    "        DATASET_CLUSTER_COLUMN_INDEX, \\\n",
    "        DATASET_DATA_COLUMNS_INDICES = METADATA[NAME]\n",
    "    DATASET_PATH_FULL = DATASET_PATH + DATASET_NAME + DATASET_EXTENSION\n",
    "else:\n",
    "    DATASET_PATH = 'data/'\n",
    "    DATASET_NAME = 'dummy'\n",
    "\n",
    "\n",
    "def generate_cluster(n_cluster, nb_features=NB_FEATURES, d=DOMAIN_LENGTH,\n",
    "                     dev_max=DEV_MAX, method='byhand'):\n",
    "    \"\"\"\n",
    "    The 'method' argument can be one of the following:\n",
    "    - 'byhand'\n",
    "    - 'multinormal'\n",
    "    \"\"\"\n",
    "    mean = np.random.random(nb_features) * d\n",
    "    if method == 'multinormal':\n",
    "        # /!\\ does not work: data does not seem random at all, covariance is\n",
    "        # always positive...!\n",
    "        raise DeprecationWarning(\"beware, 'multinormal' method does not seem\"\n",
    "                                 \"to work\")\n",
    "        cov = np.tril(np.random.random((nb_features, n_cluster)) *\n",
    "                      np.random.random() * dev_max)\n",
    "        cov = cov @ cov.transpose()  # a covariance matrix\n",
    "        return(np.random.multivariate_normal(mean, cov, n_cluster))\n",
    "    else:\n",
    "        if method != 'byhand':\n",
    "            print(\"generate_cluster: method unknown, using 'byhand'\")\n",
    "        st_devs = dev_max * np.random.random(nb_features)\n",
    "        # holds the st_devs of each feature\n",
    "        cluster_points = np.zeros((n_cluster, nb_features))\n",
    "        for i in range(n_cluster):\n",
    "            for j in range(nb_features):\n",
    "                cluster_points[i][j] = np.random.normal(loc=mean[j],\n",
    "                                                        scale=st_devs[j])\n",
    "        cluster_points = cluster_points @ special_ortho_group.rvs(nb_features)\n",
    "        return(cluster_points)\n",
    "\n",
    "\n",
    "def generate_dataset(nb_groups=NB_GROUPS, n=N, nb_features=NB_FEATURES,\n",
    "                     d=DOMAIN_LENGTH, dev_max=DEV_MAX):\n",
    "    group_sizes = np.random.random(nb_groups)\n",
    "    group_sizes *= n / np.sum(group_sizes)\n",
    "    group_sizes = np.trim_zeros(np.round(group_sizes)).astype(int)\n",
    "    data = [generate_cluster(n_cluster) for n_cluster in group_sizes]\n",
    "    data = np.vstack(data)\n",
    "    clusters_true = np.concatenate([n_cluster * [i] for i, n_cluster in\n",
    "                                    enumerate(group_sizes)])\n",
    "    np.save(DATASET_PATH + DATASET_NAME + '_data.npy', data)\n",
    "    np.save(DATASET_PATH + DATASET_NAME + '_clusters_true.npy', clusters_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate or load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_LOAD_DATASET:\n",
    "    start, end = DATASET_DATA_COLUMNS_INDICES\n",
    "    df1 = pd.read_csv(DATASET_PATH_FULL)\n",
    "    df1_np = df1.to_numpy(copy=True)\n",
    "    data = df1_np[:, start:end].astype('float')\n",
    "    clusters_true = df1_np[:, DATASET_CLUSTER_COLUMN_INDEX]\n",
    "else:\n",
    "    generate_dataset()\n",
    "    data = np.load(DATASET_PATH + DATASET_NAME + '_data.npy')\n",
    "    clusters_true = np.load(DATASET_PATH + DATASET_NAME +\n",
    "                            '_clusters_true.npy').astype(int)\n",
    "\n",
    "assert data is not None, 'data is None'\n",
    "assert clusters_true is not None, 'clusters_true is None'\n",
    "\n",
    "# print(data)\n",
    "# print(clusters_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"d\" + str(i) for i in range(data.shape[1])] + ['true cluster']\n",
    "df = pd.DataFrame(np.hstack((data, np.reshape([clusters_true],\n",
    "                                              (data.shape[0], 1)))),\n",
    "                  columns=columns)\n",
    "true_data_plot = sns.pairplot(df, kind=\"scatter\", hue='true cluster',\n",
    "                              vars=columns[:-1])\n",
    "true_data_plot.savefig(DATASET_PATH + DATASET_NAME + '_true.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastAgent(object):\n",
    "    def __init__(self,\n",
    "                 method_find='byhand',\n",
    "                 method_multivariate='mmcd',\n",
    "                 method_univariate='mad',\n",
    "                 alpha=0.95,\n",
    "                 order=2,\n",
    "                 sensitivenesses=[1, 10, 0.1],\n",
    "                 shuffleToggle=False,\n",
    "                 verbose=False):\n",
    "\n",
    "        # --- user-defined parameters ---\n",
    "\n",
    "        self.method_find = method_find\n",
    "        self.method_multivariate = method_multivariate\n",
    "        self.method_univariate = method_univariate\n",
    "        self.order = order\n",
    "        self.shuffleToggle = shuffleToggle\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # # --- 'byhand' method specific paramters ---\n",
    "        self.sensitiveness_find_cluster = sensitivenesses[0]\n",
    "        # if a point is alone with a radius of\n",
    "        # sensitiveness_find_cluster * stdist, create a new cluster\n",
    "\n",
    "        self.sensitiveness_inf_dims = sensitivenesses[1]\n",
    "        # sensitiveness to determine if a dimension is too variable to be\n",
    "        # relevant for the cluster\n",
    "\n",
    "        self.sensitiveness_zero_dims = sensitivenesses[2]\n",
    "        # sensitiveness to determine if a dimension is too concentrated to be\n",
    "        # relevant for the cluster\n",
    "\n",
    "        # # --- 'maha' method specific parameters ---\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # --- self-initialized parameters ---\n",
    "        self.allZeros = True\n",
    "        self.clusters = np.array([])\n",
    "        # clusters[i] == j means that point i belongs to cluster j\n",
    "        self.cluster_sizes = []\n",
    "        self.data = np.array([[]])\n",
    "        self.first_time = True  # is True if nb\n",
    "        self.nb_clusters = 0\n",
    "        self.nb_seen = 0\n",
    "        self.permutation = None\n",
    "        self.stdist = 0\n",
    "        self.stdists_per_cluster = []\n",
    "\n",
    "    def assign_to_cluster(self, pi, cluster_id):\n",
    "        old_cluster_id = self.clusters[pi]\n",
    "        if old_cluster_id != -1:\n",
    "            if self.cluster_sizes[old_cluster_id] == 1:\n",
    "                if old_cluster_id != self.nb_clusters - 1:\n",
    "                    # put last cluster in its place\n",
    "                    self.clusters = np.where(self.clusters != old_cluster_id,\n",
    "                                             self.clusters,\n",
    "                                             old_cluster_id)\n",
    "                    self.cluster_sizes[old_cluster_id] = \\\n",
    "                        self.cluster_sizes[-1]\n",
    "                self.cluster_sizes.pop()\n",
    "                self.nb_clusters -= 1\n",
    "            else:\n",
    "                self.cluster_sizes[old_cluster_id] -= 1\n",
    "        self.clusters[pi] = cluster_id\n",
    "        self.cluster_sizes[cluster_id] += 1\n",
    "\n",
    "    def clusterize_online(self):\n",
    "        assert len(self.data), \"empty data\"\n",
    "        if self.nb_clusters == 0:\n",
    "            self.new_cluster(0)\n",
    "            self.one_more_seen()\n",
    "        for i, p in enumerate(self.data[self.nb_seen:], start=self.nb_seen):\n",
    "            self.find_cluster(i, p, until=i)\n",
    "            if not self.allZeros:\n",
    "                self.one_more_seen()\n",
    "\n",
    "    def feed_data(self, d):\n",
    "        \"\"\"Adds data to the agent's memory\"\"\"\n",
    "        d2 = np.copy(d)\n",
    "        if self.shuffleToggle:\n",
    "            d2 = self.shuffle(d2)\n",
    "        if self.nb_clusters == 0:\n",
    "            self.data = np.copy(d2)\n",
    "            self.clusters = np.zeros(len(d2), dtype=int)\n",
    "            self.clusters.fill(-1)\n",
    "        else:\n",
    "            new_data = np.vstack((self.data, d2))\n",
    "            self.data = new_data\n",
    "            new_clusters = np.zeros(len(data), dtype=int)\n",
    "            new_clusters.fill(-1)\n",
    "            new_clusters_all = np.hstack((self.clusters, new_clusters))\n",
    "            self.clusters = new_clusters_all\n",
    "\n",
    "    def find_cluster(self, pi, p, until=None, recollection=False):\n",
    "        \"\"\"\n",
    "        The 'method' argument can be on the following:\n",
    "        'byhand', 'byhand-naive', 'maha', 'cook', 'mmcd'.\n",
    "        \"\"\"\n",
    "        method = self.method_find\n",
    "        if method in ['maha', 'cook', 'mmcd']:\n",
    "            global_ratio = self.get_outlier_ratio(p, self.data)\n",
    "            if pi == 1 or global_ratio > 1:\n",
    "                self.new_cluster(pi)\n",
    "            else:\n",
    "                cluster_ratios = []\n",
    "                for j, size in enumerate(self.cluster_sizes):\n",
    "                    if size != 0:\n",
    "                        cluster_points = self.get_cluster_points(j)\n",
    "                        ratio = self.get_outlier_ratio(p, cluster_points)\n",
    "                        cluster_ratios.append([j, ratio])\n",
    "                cluster_ratios = np.array(cluster_ratios)\n",
    "                ratios_sorted = np.argsort(cluster_ratios[:, 1])\n",
    "                bci = ratios_sorted[0]\n",
    "                bc = cluster_ratios[bci, 0]\n",
    "                ratio_best = cluster_ratios[bci, 1]\n",
    "#                 bci_second = ratios_sorted[1]\n",
    "#                 bc_second = cluster_ratios[bci_second, 0]\n",
    "#                 ratio_best_second = cluster_ratios[bci_second, 1]\n",
    "                if ratio_best > 1:\n",
    "                    self.new_cluster(pi)\n",
    "                else:\n",
    "                    self.assign_to_cluster(pi, bc)\n",
    "                    if self.verbose:\n",
    "                        print(cluster_ratios)\n",
    "                        print(\"Best cluster is {} with a ratio of {}\".format(\n",
    "                            bc, cluster_ratios[bci, 1]))\n",
    "                        print(\"{} -> cluster of {}\".format(pi, bc))\n",
    "        elif method == 'byhand-naive':\n",
    "            cluster_dims = [self.get_cluster_relevant_dimensions(ci)\n",
    "                            for ci in range(self.nb_clusters)]\n",
    "            distances = np.array([self.get_distance(p, q, self.order, cluster_dims[self.clusters[qi]][2])  # noqa\n",
    "                                  for qi, q in enumerate(self.data[:until])])\n",
    "            self.allZeros = self.allZeros and np.all(distances == 0)\n",
    "            dist_min = np.min(np.trim_zeros(distances)) if not self.allZeros \\\n",
    "                else 0\n",
    "            closest = np.argmin(distances)\n",
    "            if dist_min > self.sensitiveness_find_cluster * self.stdist and \\\n",
    "                    0 not in distances:\n",
    "                self.new_cluster(pi)\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\"{} -> cluster of {}\".format(pi, closest))\n",
    "                self.assign_to_cluster(pi, self.clusters[closest])\n",
    "                if not self.allZeros:\n",
    "                    self.update_stdists_per_cluster(self.clusters[pi], p)\n",
    "            if not recollection and not self.allZeros:\n",
    "                self.update_stdist(self.nb_seen, dist_min)\n",
    "        else:\n",
    "            if method != 'byhand':\n",
    "                print(\"find_cluster: unknown method, using 'byhand'\")\n",
    "            cluster_indexes = []\n",
    "            cluster_dims = []\n",
    "            cluster_good_ratios_sum = []\n",
    "            cluster_bad_ratios_sum = []\n",
    "            for j in self.clusters:\n",
    "                if j != -1:\n",
    "                    relevant_dims, good_ratios_sum, bad_ratios_sum, ratios = \\\n",
    "                            self.get_cluster_analysis(j, p)\n",
    "                    cluster_indexes.append(j)\n",
    "                    cluster_dims.append(relevant_dims)\n",
    "                    cluster_good_ratios_sum.append(good_ratios_sum)\n",
    "                    cluster_bad_ratios_sum.append(bad_ratios_sum)\n",
    "            ind = np.lexsort((cluster_indexes,\n",
    "                              cluster_bad_ratios_sum,\n",
    "                              cluster_good_ratios_sum,\n",
    "                              -np.array(cluster_dims)))\n",
    "            closest = cluster_indexes[ind[0]]\n",
    "            if self.verbose:\n",
    "                print([(cluster_dims[j],\n",
    "                        cluster_good_ratios_sum[j],\n",
    "                        cluster_bad_ratios_sum[j],\n",
    "                        cluster_indexes[j])\n",
    "                       for j in ind[0: min(len(ind), 3)]])\n",
    "            if cluster_dims[ind[0]] == 0 or \\\n",
    "                    self.nb_clusters == 1:\n",
    "                self.new_cluster(pi)\n",
    "            else:\n",
    "                closest_second = cluster_indexes[ind[1]]\n",
    "                cluster_fusion = np.vstack([\n",
    "                    self.get_cluster_points(closest),\n",
    "                    self.get_cluster_points(closest_second)\n",
    "                    ])\n",
    "                ratio_fusion = self.get_outlier_ratio(\n",
    "                        p, cluster_fusion, alpha=self.alpha,\n",
    "                        method=self.method_multivariate)\n",
    "                if ratio_fusion > 10:\n",
    "                    self.new_cluster(pi)\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print(\"{} -> cluster of {}\".format(pi, closest))\n",
    "                    self.assign_to_cluster(pi, self.clusters[closest])\n",
    "\n",
    "    def get_cluster_analysis(self, pi, p, custom_points=[]):\n",
    "        relevant_dims = 0\n",
    "        good_ratios_sum = 0\n",
    "        bad_ratios_sum = 0\n",
    "        ratios = self.get_outlier_ratio_per_dimension(pi, p, custom_points)\n",
    "        for r in ratios:\n",
    "            if r < 1:\n",
    "                relevant_dims += 1\n",
    "                good_ratios_sum += r\n",
    "            else:\n",
    "                bad_ratios_sum += r\n",
    "        return(relevant_dims, good_ratios_sum, bad_ratios_sum, ratios)\n",
    "\n",
    "    def get_cluster_relevant_dimensions(self, ci):\n",
    "        \"\"\" return (relevant_dims, inf_dims, zero_dims) \"\"\"\n",
    "        assert ci >= 0 and ci < self.nb_clusters, \\\n",
    "            \"Incorrect ci parameter: {}\".format(ci)\n",
    "        stdist_ci = self.stdists_per_cluster[ci]\n",
    "        cluster_points = self.get_cluster_points(ci)\n",
    "        relevant_dims, inf_dims, zero_dims = [], [], []\n",
    "        for j in range(cluster_points.shape[1]):\n",
    "            stdev = np.std(cluster_points[:, j])\n",
    "            if stdev > self.sensitiveness_inf_dims * stdist_ci:\n",
    "                inf_dims.append(j)\n",
    "            elif stdev < self.sensitiveness_zero_dims * stdist_ci:\n",
    "                zero_dims.append(j)\n",
    "            else:\n",
    "                relevant_dims.append(j)\n",
    "        return relevant_dims, inf_dims, zero_dims\n",
    "\n",
    "    def get_cluster_points(self, ci):\n",
    "        return self.data[np.nonzero(self.clusters == ci)]\n",
    "\n",
    "    def get_distance(self, p, q, order=2, dimensions=None):\n",
    "        if dimensions is not None and len(dimensions) > 0:\n",
    "            print(\"distance on dimensions {}\".format(dimensions))\n",
    "            return(np.linalg.norm(p[dimensions]-q[dimensions], order))\n",
    "        else:\n",
    "            return(np.linalg.norm(p-q, order))\n",
    "\n",
    "    # not used\n",
    "    def get_nb_obs_per_dimension(self, ci):\n",
    "        \"\"\" returns the number of distinct observations per dimension \"\"\"\n",
    "        cluster_points = self.get_cluster_points(ci)\n",
    "        d = cluster_points.shape[1]\n",
    "        nb_obs = np.zeros(d)\n",
    "        for j in range(d):\n",
    "            nb_obs[j] = len(np.unique(cluster_points[:, j]))\n",
    "        return nb_obs\n",
    "\n",
    "    def get_outlier_ratio_per_dimension(self, ci, p, custom_points=[]):\n",
    "        cluster_points = self.get_cluster_points(ci)\n",
    "        if len(custom_points) > 0:\n",
    "            cluster_points = custom_points\n",
    "        d = cluster_points.shape[1]\n",
    "        if d == 1:\n",
    "            np.hstack(cluster_points, np.mean(cluster_points,\n",
    "                                              np.reshape(p, (-1, 1))))\n",
    "            # arbitrary choice: we add the mean to get a more relevant result\n",
    "        ratios = np.zeros(d)\n",
    "        for j in range(d):\n",
    "            ratios[j] = self.get_outlier_ratio_univariate(\n",
    "                    p[j], cluster_points[:, j], method=self.method_univariate)\n",
    "        return ratios\n",
    "\n",
    "    def get_outlier_ratio(self, p, cluster_points_without_p, alpha=0.95,\n",
    "                          method='maha', h=0.75, m=5):\n",
    "        \"\"\"\n",
    "        The 'method' argument can be either of the following:\n",
    "        - 'maha': Mahalanobis distance, generalization of the distance to\n",
    "                  the mean in terms of standard deviations for\n",
    "                  multivariate data\n",
    "        - 'cook': TODO\n",
    "        - 'mmcd': TODO\n",
    "        If return value > 1, consider p an outlier\n",
    "        \"\"\"\n",
    "        eps = np.mean(cluster_points_without_p[0]) * 1e-6\n",
    "        # used to avoid singular matrices\n",
    "        cluster_points = np.vstack([p, cluster_points_without_p])\n",
    "        n = len(cluster_points)\n",
    "        threshold = chi2.isf(1-alpha, len(p))\n",
    "        if method == 'cook':\n",
    "            raise NotImplementedError(\"'cook' method not yet implemented\")\n",
    "        elif method == 'mmcd':\n",
    "            sample_size = int(h * n)\n",
    "            permutations = np.zeros((m, sample_size), dtype=int)\n",
    "            determinants = np.zeros(m)\n",
    "            for i in range(m):\n",
    "                permutations[i] = np.random.permutation(n)[:sample_size]\n",
    "                for j in range(m):\n",
    "                    sample = cluster_points[permutations[i]]\n",
    "                    sigma = np.cov(sample, rowvar=False) + \\\n",
    "                        eps * np.eye(len(p))\n",
    "                    mu = np.mean(sample, axis=0)\n",
    "                    distances = [mahalanobis(point, mu, np.linalg.inv(sigma))\n",
    "                                 for point in cluster_points]\n",
    "                    permutations[i] = np.argsort(distances)[:sample_size]\n",
    "                determinants[i] = np.linalg.det(sigma)\n",
    "            best = np.argmin(determinants)\n",
    "            sample = cluster_points[permutations[best]]\n",
    "            sigma = np.cov(sample, rowvar=False) + eps * np.eye(len(p))\n",
    "            mu = np.mean(sample, axis=0)\n",
    "            d = mahalanobis(p, mu, np.linalg.inv(sigma))\n",
    "            ratio = d / threshold\n",
    "            return ratio\n",
    "        else:\n",
    "            if method != 'maha':\n",
    "                print(\"Unknown method, using Mahalanobis distance\")\n",
    "            sigma = np.cov(cluster_points, rowvar=False) + eps * np.eye(len(p))\n",
    "            mu = np.mean(cluster_points, axis=0)\n",
    "            d = mahalanobis(p, mu, np.linalg.inv(sigma))\n",
    "            ratio = d / threshold\n",
    "            return ratio\n",
    "\n",
    "    def get_outlier_ratio_univariate(self, p, cluster_points_without_p,\n",
    "                                     B=1.4826, threshold=3, method='mad'):\n",
    "        \"\"\"\n",
    "        The 'method' argument can be one of the following:\n",
    "        - 'mean': mean +- three stdevs\n",
    "        - 'mad': median absolute deviation, more robust to detect outliers\n",
    "        If the value returned is > 1, consider p an outlier.\n",
    "        \"\"\"\n",
    "        cluster_points = np.hstack([cluster_points_without_p, [p]])\n",
    "        if method == 'mean':\n",
    "            raise NotImplementedError(\"'mean' method not yet implemented\")\n",
    "        else:\n",
    "            if method != 'mad':\n",
    "                print(\"Unknown method_univariate, using median \"\n",
    "                      \"absolute deviation\")\n",
    "            m = np.median(cluster_points)\n",
    "            mad = B * np.median(np.abs(cluster_points - m))\n",
    "            if np.abs(p-m) == 0:\n",
    "                return 0\n",
    "            elif mad == 0:\n",
    "                return np.inf\n",
    "            else:\n",
    "                return (np.abs(p-m)/(threshold*mad))\n",
    "\n",
    "    def new_cluster(self, pi):\n",
    "        if self.clusters[pi] == -1 or \\\n",
    "                not self.cluster_sizes[self.clusters[pi]] == 1:\n",
    "            if self.verbose:\n",
    "                print(\"{} -> new cluster\".format(pi))\n",
    "            if self.clusters[pi] != -1:\n",
    "                self.cluster_sizes[self.clusters[pi]] -= 1\n",
    "            self.clusters[pi] = self.nb_clusters\n",
    "            self.nb_clusters += 1\n",
    "            self.cluster_sizes.append(1)\n",
    "            self.stdists_per_cluster.append(0)\n",
    "\n",
    "    def one_more_seen(self):\n",
    "        self.nb_seen += 1\n",
    "\n",
    "    def print_clusters(self):\n",
    "        columns = [\"d\" + str(i) for i in range(self.data.shape[1])] \\\n",
    "                  + ['affected cluster']\n",
    "        df_1 = pd.DataFrame(np.hstack((self.data,\n",
    "                                       np.reshape([self.clusters],\n",
    "                                                  (self.data.shape[0], 1)))),\n",
    "                            columns=columns)\n",
    "        agent_plot = sns.pairplot(df_1, kind=\"scatter\", hue=\"affected cluster\",\n",
    "                                  vars=columns[:-1])\n",
    "        agent_plot.savefig(DATASET_PATH + DATASET_NAME + '_agent.png')\n",
    "\n",
    "    def print_scores(self):\n",
    "        pass\n",
    "\n",
    "    def shuffle(self, data_to_shuffle):\n",
    "        assert self.shuffleToggle, \\\n",
    "            \"agent: shuffle method called but shuffleToggle is False\"\n",
    "        new_permutation = np.random.permutation(len(data_to_shuffle))\n",
    "        if self.permutation is None:\n",
    "            self.permutation = new_permutation\n",
    "        else:\n",
    "            new_permutation2 = new_permutation + len(self.permutation)\n",
    "            self.permutation = np.concatenate([self.permutation,\n",
    "                                               new_permutation2])\n",
    "        return(data_to_shuffle[new_permutation])\n",
    "\n",
    "    def update_clusters(self, until_update=None, until_dist=None,\n",
    "                        recollection=True):\n",
    "        for pi, p in enumerate(self.data[:until_update]):\n",
    "            self.find_cluster(pi, p, until=until_dist,\n",
    "                              recollection=recollection)\n",
    "\n",
    "    def update_stdist(self, nb_seen, dist):\n",
    "        old_dist = self.stdist\n",
    "        self.stdist = (max(1, nb_seen-1) * self.stdist + dist) / (nb_seen)\n",
    "        if self.verbose:\n",
    "            print('distance = {}, self.stdist = {} -> {}'.format(\n",
    "                dist, old_dist, self.stdist))\n",
    "\n",
    "    def update_stdists_per_cluster(self, ci, p):\n",
    "        old_dist = self.stdists_per_cluster[ci]\n",
    "        relevant_dims, inf_dims, zero_dims = \\\n",
    "                self.get_cluster_relevant_dimensions(ci)\n",
    "        if len(relevant_dims):\n",
    "            cluster_points_rd = self.get_cluster_points(ci)[:, relevant_dims]\n",
    "            p_rd = p[relevant_dims]\n",
    "        elif len(inf_dims):\n",
    "            cluster_points_rd = self.get_cluster_points(ci)[:, inf_dims]\n",
    "            p_rd = p[inf_dims]\n",
    "        else:\n",
    "            cluster_points_rd = self.get_cluster_points(ci)[:, zero_dims]\n",
    "            p_rd = p[zero_dims]\n",
    "        array_dists = np.array([self.get_distance(p_rd, q, self.order) for q in\n",
    "                                cluster_points_rd])\n",
    "#         print(relevant_dims, inf_dims, zero_dims)\n",
    "#         print(array_dists)\n",
    "        dist_p = np.min(array_dists)\n",
    "        csize = cluster_points_rd.shape[0]\n",
    "        self.stdists_per_cluster[ci] = (old_dist * csize + dist_p) / (csize + 1)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final stdist: 15.707233301629234\n",
      "All points in a cluster? True\n",
      "Number of clusters: 6\n",
      "[[ 0  1  2  3  4  5]\n",
      " [ 5 27  9  8 19 12]]\n"
     ]
    }
   ],
   "source": [
    "ca = ContrastAgent(method_find='byhand-naive', sensitivenesses=[1.5, 10, 0.1],\n",
    "                   shuffleToggle=True, verbose=False, alpha=0.9)\n",
    "ca.feed_data(data)\n",
    "ca.clusterize_online()\n",
    "if ca.method_find == 'byhand-naive':\n",
    "    print(\"Final stdist: {}\".format(ca.stdist))\n",
    "print(\"All points in a cluster? {}\".format(-1 not in ca.clusters))\n",
    "print(\"Number of clusters: {}\".format(len(unique_labels(ca.clusters))))\n",
    "unique, counts = np.unique(ca.clusters, return_counts=True)\n",
    "print(np.array([unique, counts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.print_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(confusion_matrix_only=True):\n",
    "    if clusters_true is not None:\n",
    "        # clusters_true2 will contain ids instead of labels\n",
    "        ids = collections.defaultdict(functools.partial(next,\n",
    "                                                        itertools.count()))\n",
    "        clusters_true2 = np.array([ids[label] for label in clusters_true])\n",
    "        if ca.shuffleToggle:\n",
    "            clusters_true2 = clusters_true2[ca.permutation]\n",
    "\n",
    "    if clusters_true is not None:\n",
    "        class_names = np.array([str(i) for i in range(1+np.max(\n",
    "            np.concatenate([clusters_true2, ca.clusters])))])\n",
    "        plot_confusion_matrix(\n",
    "            clusters_true2, ca.clusters, classes=class_names,\n",
    "            normalize=True, title='Normalized confusion matrix',\n",
    "            path=DATASET_PATH + DATASET_NAME)\n",
    "\n",
    "    if confusion_matrix_only:\n",
    "        return\n",
    "\n",
    "    if clusters_true is not None:\n",
    "        print(\"--- Metrics involving ground truth--- \")\n",
    "        print(\"Adjusted Rand Index: {}\".format(\n",
    "            metrics.adjusted_rand_score(clusters_true2, ca.clusters)))\n",
    "        print(\"Adjusted Mutual Information score: {}\".format(\n",
    "            metrics.adjusted_mutual_info_score(clusters_true2, ca.clusters)))\n",
    "        print(\"Homogeneity: {}\".format(\n",
    "            metrics.homogeneity_score(clusters_true2, ca.clusters)))\n",
    "        print(\"Completeness: {}\".format(\n",
    "            metrics.completeness_score(clusters_true2, ca.clusters)))\n",
    "        print(\"V-measure score: {}\".format(\n",
    "            metrics.v_measure_score(clusters_true2, ca.clusters)))\n",
    "        print(\"Fowlkes-Mallows score: {}\".format(\n",
    "            metrics.fowlkes_mallows_score(clusters_true2, ca.clusters)))\n",
    "        print()\n",
    "\n",
    "    print(\"--- Metrics not involving ground truth ---\")\n",
    "    if clusters_true is not None:\n",
    "        print(\"Silhouette score (original): {}\".format(\n",
    "            metrics.silhouette_score(data, clusters_true)))\n",
    "    print(\"Silhouette score (agent): {}\".format(\n",
    "        metrics.silhouette_score(ca.data, ca.clusters)))\n",
    "    if clusters_true is not None:\n",
    "        print(\"Calinski-Harabaz score (original): {}\".format(\n",
    "            metrics.calinski_harabaz_score(data, clusters_true)))\n",
    "    print(\"Calinski-Harabaz score (agent): {}\".format(\n",
    "        metrics.calinski_harabaz_score(ca.data, ca.clusters)))\n",
    "    if clusters_true is not None:\n",
    "        print(\"Davies-Bouldin score (original): {}\".format(\n",
    "            metrics.davies_bouldin_score(data, clusters_true)))\n",
    "    print(\"Davies-Bouldin score (agent): {}\".format(\n",
    "        metrics.davies_bouldin_score(ca.data, ca.clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca.update_clusters()\n",
    "# print_metrics()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
